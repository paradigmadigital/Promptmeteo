{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b559000b-5af7-49ed-8e0a-c9c4765167f0",
   "metadata": {},
   "source": [
    "# Promptmeteo Usage - Save and Load Model\n",
    "\n",
    "LLMs are truly revolutionizing the world, enabling humans to do things we couldn't do before, or making them so much easier and faster to do.\n",
    "\n",
    "Promptmeteo leverages the power of LLMs to democratize the data science process. This means you can easily train a model and make predictions with it in a few, comfortable steps.\n",
    "\n",
    "In this tutorial, you are going to create a model to perform document classification (a classic NLP task), train it with a manageable amount of data, save it, load it and use it to classify new documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d34032-bdaa-4317-94ef-432793c2a1dd",
   "metadata": {},
   "source": [
    "## 1. Create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404b619-fdbb-4306-be6c-d145c4de1043",
   "metadata": {},
   "source": [
    "### 1.1 Instance a DocumentClassifier\n",
    "Luckily, Promptmeteo has a specific task to do document classification. You only have to instance the class DocumentClassifier, and it's very easy to do so. You have to set:\n",
    "- `language`: the language in which the task is going to be performed (currently, only English or Spanish are supported)\n",
    "- `model_name` and `model_provider_name`: the model and provider you want to use (currently, [these](https://paradigmadigital.github.io/promptmeteo-docs/definition.html#available-models) are supported)\n",
    "- `prompt_labels`: the categories in which you need to classify the documents (free choice)\n",
    "\n",
    "Let's use `google/flan-t5-small`, which is free, in English, and go for the classic sentiment classification, that is, using the `positive` and `negative` labels to classify opinions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4106fe4b-4ee3-4763-a54a-7dcfe9febc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from promptmeteo import DocumentClassifier\n",
    "\n",
    "clf = DocumentClassifier(\n",
    "        language            = 'en',\n",
    "        model_provider_name = 'hf_pipeline',\n",
    "        model_name          = 'google/flan-t5-small',\n",
    "        prompt_labels       = ['positive', 'negative']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d8729-635c-4ee9-b59f-d3a9646cf253",
   "metadata": {},
   "source": [
    "### 1.2 Prompts\n",
    "The first thing you need to know about Promptmeteo is that we think **prompts should be treated with the same respect as code**. Therefore, they shouldn't be hard-coded or taken lightly; instead, they should be carefully designed and properly versioned. In the end, they are the way we communicate with the LLMs. That means a mistake in a prompt can result in major unwanted outputs, while a really good prompt can unleash all the power from the LLM.\n",
    "\n",
    "That said, prompt templates in Promptmeteo are written in YAML and saved in files. So first, we need to write a helper function to print YAML files in a proper way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c6baaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def prompt_print(prompt: str):\n",
    "    \"\"\"Prints YAML prompts in a nice way.\n",
    "    \"\"\"\n",
    "    yaml_prompt = yaml.safe_load(prompt)\n",
    "    for key, value in yaml_prompt.items():\n",
    "        print(key, \"\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bed89f-1fea-4d23-a56a-1076990147c4",
   "metadata": {},
   "source": [
    "#### 1.2.1 Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533b297-de29-4e55-8404-1d58c424db5f",
   "metadata": {},
   "source": [
    "Promptmeteo has predefined **prompt templates** for each of the available languages, models and tasks. This allows the user not to wonder about defining the perfect prompt, but rather to parametrize the prompt template with the parameters of the use case, without neglecting the prompts.\n",
    "\n",
    "Let's have a look into the prompt template for the document classification task in English with FlanTL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29a6f79c-95ca-4fe9-a945-6c8379888d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPLATE \n",
      " I need you to help me with a text classification task. {__PROMPT_DOMAIN__} {__PROMPT_LABELS__}\n",
      "{__CHAIN_THOUGHT__} {__ANSWER_FORMAT__} {__SHOT_EXAMPLES__} {__PROMPT_SAMPLE__}\n",
      "PROMPT_DOMAIN \n",
      " The texts you will be processing are from the {__DOMAIN__} domain.\n",
      "PROMPT_LABELS \n",
      " I want you to classify the texts into one of the following categories: {__LABELS__}.\n",
      "PROMPT_DETAIL \n",
      " \n",
      "SHOT_EXAMPLES \n",
      " Examples:\n",
      "\n",
      "{__EXAMPLES__}\n",
      "PROMPT_SAMPLE \n",
      " \n",
      "\n",
      "{__SAMPLE__}\n",
      "\n",
      "CHAIN_THOUGHT \n",
      " \n",
      "ANSWER_FORMAT \n",
      " In your response, include only the name of the class predicted.\n"
     ]
    }
   ],
   "source": [
    "prompt_print(clf.task.prompt.PROMPT_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9379c-fcfe-4736-84be-47ce75196afb",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b81ee-2bac-4e20-9860-9670fddf1b55",
   "metadata": {},
   "source": [
    "#### 1.2.2 Prompt texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d81af-4601-4366-94f5-09bc9039890b",
   "metadata": {},
   "source": [
    "The prompt template is used to build a **prompt text**, which is the final text to pass to the LLM. We haven't provided any example yet, only the labels, so it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a89220b2-de0d-48a3-9f80-d624f5d877d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need you to help me with a text classification task. The texts you will be processing are from the  domain. I want you to classify the texts into one of the following categories: positive, negative.\n",
      " In your response, include only the name of the class predicted. Examples:\n",
      "\n",
      "{__EXAMPLES__} \n",
      "\n",
      "{__SAMPLE__}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clf.task.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5bf76-0eb0-442e-9f45-77472a7acb70",
   "metadata": {},
   "source": [
    "We could use the above text as the prompt as is, it's a valid approach called **zero-shot prompting**. But usually we will need to show the LLM some examples of what we want it to do; and that means we need to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe750dc3-ba7d-4403-bd1a-68e2e6177718",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37bca87-fa99-4143-9564-e7ae9364510a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8098ae-bad4-43f3-8c0b-4f7cceab8b6a",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c362572-35c8-4319-bed5-517484430d76",
   "metadata": {},
   "source": [
    "## 2. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba3d55-1061-43b7-acfd-13ff8d83370f",
   "metadata": {},
   "source": [
    "### 2.1 `train()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707a324-78a1-4b58-b318-5f2808eb3620",
   "metadata": {},
   "source": [
    "As in any data science pipeline, you can train models in Promptmeteo. You simply have to give examples to it using the `train()` function, providing the texts (`examples`) and their expected classification (`annotations`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3141d3-8c0b-4dac-a472-cccd4c3d4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.train(\n",
    "    examples    = ['i am happy', 'I like it', 'I hate it'],\n",
    "    annotations = ['positive', 'positive', 'negative'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbbf2a-c7ee-43d3-a445-2a94f53a4301",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df6a6d-4d38-4e42-a045-22d7e463a224",
   "metadata": {},
   "source": [
    "### 2.2 Examples injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591ccbb-25ab-4610-9f9d-9e4a6313cac2",
   "metadata": {},
   "source": [
    "Each example is added to the prompt to help the model improve the answers. When the number of examples is low, this technique is called **few-shot prompting**.\n",
    "\n",
    "These examples should be chosen from those that are more related to the new sample passed for making inference. We can see that now the prompt with the examples has the following aspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "092ffb73-2bc8-42db-afb1-ca885c75962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need you to help me with a text classification task. The texts you will be processing are from the  domain. I want you to classify the texts into one of the following categories: positive, negative.\n",
      " In your response, include only the name of the class predicted. Examples:\n",
      "\n",
      "I like it\n",
      "positive\n",
      "\n",
      "I hate it\n",
      "negative\n",
      "\n",
      "i am happy\n",
      "positive \n",
      "\n",
      "I love it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_for_inference = 'I love it'\n",
    "print(clf.task._get_prompt(example_for_inference))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7f3b4-d40d-4757-91d3-823e2455c706",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711ac62-309a-441c-adfb-aa009d0cd334",
   "metadata": {},
   "source": [
    "### 2.3 Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d27b6a-3696-44d0-a234-7a36615ce524",
   "metadata": {},
   "source": [
    "The examples passed to the `train()` function are saved in a vectorstore in local (with FAISS) and this vectorstore can be serialized to disk. Saving these examples in disk allows us to easily reuse them for new use cases, without having to retrieve the original data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c748ded2-28a3-4e03-a813-f62e454f16c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<promptmeteo.document_classifier.DocumentClassifier at 0x7ff4221a8ee0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.save_model('my_classifier.meteo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a2ae5-8745-4e23-8167-b3d0fea79c61",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb332b-2a95-4733-b221-03909f059e5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e50993-018e-4fac-93f7-5f4db4fbb369",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5544d1b-eacf-4deb-a04c-22627123ce98",
   "metadata": {},
   "source": [
    "## 3. Load a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fe212-4dc9-4303-90c2-3cb670b10195",
   "metadata": {},
   "source": [
    "### 3.1 Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2c604-67c7-4834-8552-d69206762eb1",
   "metadata": {},
   "source": [
    "Now that we have saved a model, we can load it. To load a model we have to intantiate a `DocumentClassifier` as we did before and use the function `load_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cae37b63-2780-4581-8dac-89a1919c0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptmeteo import DocumentClassifier\n",
    "\n",
    "new_clf = DocumentClassifier(\n",
    "        language            = 'en',\n",
    "        model_provider_name = 'hf_pipeline',\n",
    "        model_name          = 'google/flan-t5-small',\n",
    "        prompt_labels       = ['positive', 'negative']\n",
    "    ).load_model('my_classifier.meteo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f27c6-1b71-4299-ad38-fa23230cee30",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382efddf-7688-46a7-b4ef-2955b3be8cee",
   "metadata": {},
   "source": [
    "### 3.2 Predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69571f3e-9e19-4956-bc41-2ae2aff2717e",
   "metadata": {},
   "source": [
    "And now we are ready to predict labels for new data! By calling the function `predict()` we can use the prompt created with the examples to predict the classification over new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "298b74ce-20a8-44b3-b946-65a7000f35ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['positive']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_clf.predict(['so cool!!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d96c7-601b-4391-9660-85c91a74d6d2",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd2c84-23a6-4eba-933d-89725978dc3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247bb95-6491-42bf-80a6-762c133b2c41",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d2939-3941-4714-9f69-082fa391c017",
   "metadata": {},
   "source": [
    "## 4 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa2277-0785-4143-9a88-d7cd7be528ea",
   "metadata": {},
   "source": [
    "* In this example we have shown how Promptmeteo can be used as a machine learning framework such as Scikit-Learn or Pytorch. It has a similar interface which allows to save the results from the training in a binary file and reuse it. This eases the integration of the LLM solution in ML pipeline tools such as Sagemaker or Vertex.\n",
    "\n",
    "* Promptmeteo does not only include code data to simplify the integration of LLM model and services. It also includes predefined prompt engineering logic for different models and tasks. It allows to focus on developing a solution rather than writting prompts, and ensures that the prompt has been tested correctly by Promptmeteo, which makes this kind of solution less error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfd4e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
